# Infernet ML

### What is Infernet ML?

Ritual provides easy-to-use abstractions for users to create AI/ML workflows that can be deployed on Infernet nodes.
The [`infernet-ml`](https://github.com/ritual-net/infernet-ml) library is a Python SDK that provides a set of tools and extendable classes for creating and
deploying machine learning workflows. It is designed to be easy to use, and provides a consistent interface for data pre-processing, inference, and post-processing.
pre-processing, inference, and post-processing of data.

### Batteries Included

We provide a set of pre-built workflows for common use-cases. We have workflows for running ONNX models, Torch models,
any Huggingface model via [Huggingface inference client](https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client)
and even for closed-source models such as OpenAI's GPT-4.

### Getting Started

Head over to the [next section](./quickstart.md) for installation and a quick
walkthrough of
the ML workflows.

### Tutorials & Videos

Head over to [Ritual Learn](https://learn.ritual.net) for more end-to-end tutorials and examples that use this library, including:
1. **[Prompt to NFT](https://learn.ritual.net/examples/prompt_to_nft)**: A tutorial on using Infernet to create & mint
   an NFT generated by stable diffusion from a prompt.
2. **[Running a Torch Model](https://learn.ritual.net/examples/running_a_torch_model)**: We import a Torch model,
   and use [`infernet-ml`](https://github.com/ritual-net/infernet-ml)'s workflows to invoke it either from a
   smart contract or from `infernet-node`'s REST API.
3. **[Running an ONNX Model](https://learn.ritual.net/examples/running_an_onnx_model)**: Similar to the Torch tutorial,
   we do the same with an ONNX model.
4. **[TGI Inference](https://learn.ritual.net/examples/tgi_inference_with_mistral_7b)**: A tutorial on running
   Mistral-7b on Infernet, and optionally delivering its output to a smart-contract.
5. **[GPT-4 Inference](https://learn.ritual.net/examples/running_gpt_4)**: A tutorial on how to use our
   [closed-source-inference workflow](../ml-workflows/inference-workflows/css_inference_workflow.mdx) to easily
   integrate with OpenAI's completions API.
